import requests
import os
import subprocess
import re
from dotenv import load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain_groq import ChatGroq
from typing import Dict, List, Tuple

# =====================================================
# 1. Configuration & Setup (Manual Testing Maintained)
# =====================================================
load_dotenv()
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
GROQ_API_KEY = os.getenv("API_KEY") # Kept original name for consistency
owner = os.getenv("OWNER")
repo = os.getenv("REPO")
pr_number = os.getenv("PR_NUMBER")

if not GROQ_API_KEY:
    raise ValueError("âŒ GROQ_API_KEY not found. Please set it in .env file.")
if not (GITHUB_TOKEN and owner and repo and pr_number):
    print("âš ï¸ WARNING: GitHub configuration (TOKEN, OWNER, REPO, PR_NUMBER) must be set in .env for full script execution.")
    # Allow code execution to proceed for analysis testing if diff is available

# Language-to-File-Extension Map
FILE_LANG_MAP = {
    "py": "python",
    "js": "javascript", "jsx": "javascript", "ts": "javascript", "tsx": "javascript",
    "java": "java",
    "cpp": "cpp", "cc": "cpp", "cxx": "cpp", "h": "cpp", "hpp": "cpp",
    "go": "go",
    "kt": "kotlin",
    "rs": "rust"
}

# Static Analyzer Commands Map (Targeted, requiring file paths as arguments)
ANALYZERS = {
    # Pylint, Flake8, Mypy, ESLint, etc. can accept file paths as arguments.
    "python": [
        ("ðŸ§© Pylint", ["pylint", "--exit-zero"]),
        ("ðŸŽ¯ Flake8", ["flake8", "--exit-zero"]),
        ("ðŸ”’ Bandit", ["bandit", "-r"]), # Note: Bandit -r usually scans recursively from dir, might analyze more than just files listed.
        ("ðŸ§  Mypy", ["mypy", "--ignore-missing-imports"]),
    ],
    "javascript": [
        ("ESLint", ["eslint", "--max-warnings=0"]),
        # Add TypeScript analysis here if needed, e.g., ["tsc", "--noEmit"]
    ],
    # Other examples (you must ensure these tools are installed locally for manual testing)
    "java": [("Checkstyle", ["checkstyle", "-c", "/google_checks.xml"])],
    "cpp": [("Cppcheck", ["cppcheck", "--enable=all", "--quiet"])],
    "go": [("Staticcheck", ["staticcheck"])],
    "rust": [("Clippy", ["cargo", "clippy", "--", "-D", "warnings"])]
}


# =====================================================
# 2. Fetch PR Diff from GitHub (Unchanged for Manual Testing)
# =====================================================
def fetch_pr_diff(owner, repo, pr_number, token):
    url = f"https://api.github.com/repos/{owner}/{repo}/pulls/{pr_number}"
    headers = {"Authorization": f"token {token}"}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"GitHub API Error: {response.json()}")
    diff_url = response.json()["diff_url"]
    diff = requests.get(diff_url, headers=headers).text
    return diff


# =====================================================
# 3. Optimized Static Analysis Functions
# =====================================================

def get_changed_files_and_languages(diff_text: str) -> Dict[str, List[str]]:
    """Infer file types/languages and get paths from PR diff."""
    # Finds lines that start with '+++ b/' and captures the file path
    # Ignores files in .git, node_modules, etc. by common practice, but not explicitly filtered here.
    file_paths = re.findall(r'\+\+\+ b/(.*)', diff_text)
    
    changed_files: Dict[str, List[str]] = {}
    
    for path in file_paths:
        ext = path.split('.')[-1].lower()
        lang = FILE_LANG_MAP.get(ext)
        if lang:
            # Add file path to the list for its detected language
            changed_files.setdefault(lang, []).append(path)

    return changed_files

def run_static_analysis(diff_text: str) -> str:
    """Run appropriate static analyzers on ONLY the changed files."""
    changed_files_map = get_changed_files_and_languages(diff_text)
    
    if not changed_files_map:
        return "âš ï¸ No recognizable programming language files found in PR diff to analyze."

    results: List[str] = []
    
    # Loop through each detected language and its files
    for lang, files in changed_files_map.items():
        results.append(f"=== ðŸ” Targeted Static Analysis for {lang.upper()} ({len(files)} files changed) ===")
        
        analyzer_list = ANALYZERS.get(lang, [])

        if not analyzer_list:
            results.append(f"No analyzer configured for {lang}")
            continue
            
        for name, base_cmd in analyzer_list:
            # Concatenate base command with file list
            full_cmd = base_cmd + files
            
            # Use subprocess.run for reliable error handling
            try:
                # Run the command
                process = subprocess.run(
                    full_cmd,
                    capture_output=True,
                    text=True,
                    check=False, # Do not raise exception on non-zero exit code
                    timeout=120 # Increased timeout for safety during manual testing
                )
                
                output = process.stdout.strip()
                error_output = process.stderr.strip()
                
                # Check for output or errors
                if output or error_output:
                     results.append(f"| {name}:\n```\n{output if output else error_output}\n```")
                else:
                    results.append(f"| {name}: No issues found.")

            except FileNotFoundError:
                results.append(f"| {name}: âŒ Command not found. Is the tool installed locally and in PATH?")
            except subprocess.TimeoutExpired:
                results.append(f"| {name}: âŒ Execution timed out after 120 seconds.")
            except Exception as e:
                results.append(f"| {name}: âŒ Error running analyzer: {e}")

    return "\n\n".join(results)


# =====================================================
# 4. Post Review Comment to GitHub (Unchanged for Manual Testing)
# =====================================================
def post_review_comment(owner, repo, pr_number, token, review_body):
    url = f"https://api.github.com/repos/{owner}/{repo}/issues/{pr_number}/comments"
    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github+json",
        "User-Agent": "AI-PR-Reviewer-Script"
    }
    payload = {"body": review_body}
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code not in [200, 201]:
        raise Exception(f"âŒ Failed to post comment: {response.json()}")
    return response.json()


# =====================================================
# 5. Initialize Groq LLM (AI Reviewer)
# =====================================================
llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.3,
    api_key=GROQ_API_KEY,
)
parser = StrOutputParser()

review_prompt = ChatPromptTemplate.from_messages([
    ("system",
      "You are a senior software engineer reviewing a GitHub Pull Request. "
      "You will receive the PR diff and static analysis results. "
      "The static analysis was run only on the files changed in the PR. "
      "Provide clear, concise, and technically correct review comments. "
      "Focus on correctness, maintainability, security, and readability. Use Markdown for clear formatting."
    ),
    ("human",
      "Here is the PR diff:\n\n{diff}\n\n"
      "And here are static analysis results:\n\n{static}\n\n"
      "Now provide a professional GitHub PR review. Include actionable feedback, "
      "specific file references, and improvement suggestions.")
])

review_chain = review_prompt | llm | parser

# Helper to truncate cleanly for the LLM context limit
def safe_truncate(text: str, max_len: int = 4000) -> str:
    if len(text) <= max_len:
        return text
    truncated = text[:max_len]
    last_newline = truncated.rfind('\n')
    if last_newline != -1:
        return truncated[:last_newline] + "\n\n... (Output truncated)"
    return truncated + " ... (Output truncated)"


# =====================================================
# 6. Main Logic
# =====================================================
if __name__ == "__main__":
    try:
        if not all([GITHUB_TOKEN, owner, repo, pr_number]):
            raise Exception("Cannot proceed. Missing GitHub configuration for manual testing. Please check .env.")
            
        diff_text = fetch_pr_diff(owner, repo, pr_number, GITHUB_TOKEN)
        print("âœ… Diff fetched successfully.\n")

        print("ðŸ” Detecting language and running *targeted* static analysis...")
        # The key change: static analysis now targets specific files
        static_output = run_static_analysis(diff_text)

        print("ðŸ¤– Sending diff + analyzer results to AI reviewer...\n")
        
        truncated_diff = safe_truncate(diff_text, 8000) # Increased limit for better context
        truncated_static = safe_truncate(static_output, 8000)
        
        review = review_chain.invoke({
            "diff": truncated_diff,
            "static": truncated_static
        })

        print("=== ðŸ§  AI REVIEW RESULT ===")
        print(review)
        print("===========================")

        print("ðŸ“Œ Posting review comment to GitHub...")
        comment = post_review_comment(owner, repo, pr_number, GITHUB_TOKEN, review)
        print(f"âœ… Review posted at: {comment['html_url']}")

    except Exception as e:
        print("Error:", str(e))
